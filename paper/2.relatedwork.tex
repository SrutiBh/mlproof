\section{Related Work}

\paragraph{Automatic Segmentation}

Multi-terabyte EM brain volumes require automatic segmentation~\cite{jain2010,Liu2014,NunezIglesias2013Machine,GALA2014}, but can be hard to classify \change{due to ambiguous intercellular space: the 2013 IEEE ISBI neurites 3D segmentation challenge~\cite{isbi_challenge} showed that existing algorithms which learn from expert-segmented training data still exhibit high error rates.}

NeuroProof \cite{neuroproof2013} \change{tries to decrease error rates} with interactive learning of agglomeration of over-segmentations of images, based on a random forest classifier. Vazquez-Reina et al.~\cite{amelio_segmentation} propose automatic 3D segmentation by taking whole EM volumes into account rather than a per section approach, then solving a fusion problem with a global context. Kaynig et al.~\cite{kaynig10} propose a random forest classifier coupled with an anisotropic smoothing prior in a conditional random field framework with 3D segment fusion. It is also possible to learn segmentation classification features directly from images with CNNs. Ronneberger et al.~\cite{RonnebergerFB15} use a contracting/expanding CNN path architecture to enable precise boundary localization with small amounts of training data. Lee et al.~\cite{lee2015recursive} recursively train very deep networks with 2D and 3D filters to detect boundaries. 

\change{These approaches make good progress; however, in general, proofreading is required to improve them through generating more ground-truth segmentations.}

% JT: Feels out of place...
Arganda-Carreras et al.~\cite{10.3389/fnana.2015.00142} posed the ISBI 2D EM segmentation challenge in 2012, where a 30-image corpus of fly cell `in/out' labels was used to train boundary detection. However, mouse brain EM data is more difficult than the ISBI 2012 challenge, as it contains significant intercellular space which is hard to classify.

Bogovic et al.~\cite{BogovicHJ13} learn 3D features, and show even that unsupervised learning can produce better features than hand-designs. We extend the features reported by Bogovic et al.\ but limit the classification to 2D images rather than 3D. In this case, reconstruction is unnecessary and, instead of waiting for the alignment of the 3D output, proofreading can start immediately to maximize throughput.

\paragraph{Collaborative Interactive Segmentation}

Recent works attack the problem of massive volume segmentation through crowd-sourcing\cite{saalfeld09,anderson2011}. EyeWire~\cite{eyewire2012} asks novice users to participate in a segmentation game with neuronal structures using a semi-automatic algorithm. D2P ~\cite{Giuly2013DP2} uses a micro-labor workforce approach where local boolean decisions are combined to produce a consensus segmentation. In general, our goal is to correct the output of a segmentation which is thought to be good; hence, our tool would be used after learning a segmentation model to direct user attention to correct likely erroneous areas.

\paragraph{Proofreading Tools}
\change{The editing bottleneck for existing imperfect automatic segmentations} was identified by Peng et al. \cite{proofreading_bottleneck}. The authors developed software which supports three-dimensional proofreading. Also for 3D, Sicat et al. proposed a graph abstraction method to guide users to problematic regions indicating the need for corrections \cite{markus_proofreading}. Janelia Farms then introduced Raveler \cite{raveler}, a software targeting expert users by offering many parameters for tweaking the proofreading process at the cost of a higher complexity. Raveler also includes a simple 3D renderer to validate corrections across slices.

\change{In 2014, Haehn et al.\ developed two proofreading tools: Mojo and Dojo. Mojo provides a simple scribble interface for error correction, and Dojo extends this for distributed proofreading via a minimalistic web-based user interface. The authors defined requirements for general proofreading tools, and then evaluated the accuracy and speed of Raveler, Mojo, and Dojo through a quantitative user study (Sec. 3 and 4) \cite{haehn_dojo_2014}.}
Al-Awami et al.\ integrated Dojo into the Neuroblocks proofreading management system \cite{Neuroblocks}, which tracks and visualizes progress among multiple users.

In 2015, Karimov et al.\ proposed a guided volume editing approach using histogram dissimilarity \cite{karimov_guided_volume_editing}. Measuring differences in histogram distributions helps to find potential errors and to suggest possible corrections. These promising results on Computed-Tomography Angiography datasets are targeted towards expert users.

Recently, Uzunbas et al.\ showed that potential labeling errors can be found by considering the merge tree of an automatic segmentation method \cite{uzunbas}. The authors track uncertainty through automatic labeling and then present potential regions for proofreading. This requires access to the segmentation merge tree, which is not always available.


%\subsection{Contributions}

%Given this, we contribute to the literature:
%\begin{enumerate}
%\item One contribution
%\item Two contribution
%\item (Maybe) three contribution
%\end{enumerate}